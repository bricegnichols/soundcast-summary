{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in any network_summary_detailed file in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_dir = sys.argv[1]\n",
    "# output_dir = sys.argv[2]\n",
    "input_dir = r'C:\\Users\\Brice\\surveys\\surveys\\net_summaries'\n",
    "output_dir = r'C:\\Users\\Brice\\surveys\\surveys\\net_summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up psrc time of day \n",
    "tod_list = ['5to6','6to7','7to8','8to9','9to10','10to14','14to15','16to17','17to18','18to20']\n",
    "\n",
    "tod_lookup = {  0:'20to5',\n",
    "                1:'20to5',\n",
    "                2:'20to5',\n",
    "                3:'20to5',\n",
    "                4:'20to5',\n",
    "                5:'5to6',\n",
    "                6:'6to7',\n",
    "                7:'7to8',\n",
    "                8:'8to9',\n",
    "                9:'9to10',\n",
    "                10:'10to14',\n",
    "                11:'10to14',\n",
    "                12:'10to14',\n",
    "                13:'10to14',\n",
    "                14:'14to15',\n",
    "                15:'15to16',\n",
    "                16:'16to17',\n",
    "                17:'17to18',\n",
    "                18:'18to20',\n",
    "                19:'18to20',\n",
    "                20:'18to20',\n",
    "                21:'20to5',\n",
    "                22:'20to5',\n",
    "                23:'20to5' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_csv_list = ['transit_boardings','traffic_counts','net_summary']\n",
    "\n",
    "    overwrite = True\n",
    "\n",
    "    if overwrite:\n",
    "        for fname in output_csv_list:\n",
    "            if os.path.isfile(os.path.join(output_dir,fname+'.csv')):\n",
    "                os.remove(os.path.join(output_dir,fname+'.csv'))\n",
    "\n",
    "    for fname in os.listdir(input_dir):\n",
    "        if fname.endswith('.xlsx'):\n",
    "            net_file = os.path.join(input_dir,fname)\n",
    "\n",
    "            print 'processing ' + fname\n",
    "\n",
    "            transit_summary(net_file, fname)\n",
    "            traffic_counts(net_file, fname)\n",
    "            net_summary(net_file, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv(df, fname):\n",
    "    '''\n",
    "    Write dataframe to file; append existing file\n",
    "    '''\n",
    "\n",
    "    if not os.path.isfile(os.path.join(output_dir,fname)):\n",
    "        df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "    else: # append without writing the header\n",
    "        df.to_csv(os.path.join(output_dir,fname), mode ='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transit_summary(net_file, fname):\n",
    "    \n",
    "    transit_df = pd.read_excel(net_file, sheetname='Transit Summaries')\n",
    "    transit_df.index = transit_df['route_code']\n",
    "    \n",
    "    # Add model results\n",
    "    dict_result = {}\n",
    "    for field in ['board','time']:\n",
    "        df = pd.DataFrame(transit_df[[tod+'_'+ field for tod in tod_list]].stack())\n",
    "        df.rename(columns={0:field}, inplace=True)\n",
    "        df['tod'] = [i.split('_')[0] for i in df.index.get_level_values(1)]\n",
    "        df['route_id'] = df.index.get_level_values(0)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        dict_result[field] = df\n",
    "\n",
    "    # Only keep the boardings for now - observed time data is not available at the route level\n",
    "    df = dict_result['board'].groupby(['route_id','tod']).sum()\n",
    "    df.reset_index(inplace=True)\n",
    "    df['source'] = fname.split('.xlsx')[0]\n",
    "\n",
    "    fname_out = 'transit_boardings.csv'\n",
    "    write_csv(df=df, fname=fname_out)\n",
    "            \n",
    "    # Add observed data if it doesn't already exist\n",
    "    if 'observed' not in pd.read_csv(os.path.join(output_dir,fname_out))['source'].values:\n",
    "        \n",
    "        df = pd.read_csv('data/transit_boardings_2014.csv')\n",
    "        df.index = df['PSRC_Rte_ID']\n",
    "        df.drop([u'Unnamed: 0','PSRC_Rte_ID','SignRt'],axis=1,inplace=True)\n",
    "\n",
    "        df = pd.DataFrame(df.stack())\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={0:'board', 'level_1':'hour','PSRC_Rte_ID':'route_id'}, inplace=True)\n",
    "              \n",
    "        # Convert hour to time of day definition\n",
    "        df['hour'] = df['hour'].apply(lambda row: row.split('_')[-1])\n",
    "        tod_df = pd.DataFrame(data=tod_lookup.values(),index=tod_lookup.keys(), columns=['tod'])\n",
    "        tod_df['hour'] = tod_df.index.astype('str')\n",
    "\n",
    "        df = pd.merge(df,tod_df,on='hour')\n",
    "        df.drop('hour', axis=1,inplace=True)\n",
    "        \n",
    "        # Group by tod\n",
    "        df = df.groupby(['tod','route_id']).sum()\n",
    "        df['tod'] = df.index.get_level_values(0)\n",
    "        df['route_id'] = df.index.get_level_values(1)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        df['source'] = 'observed'\n",
    "        \n",
    "        # Re-order results to match model data\n",
    "        df = df[['route_id','tod','board','source']]\n",
    "        \n",
    "        write_csv(df=df, fname=fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traffic_counts(net_file, fname):\n",
    "    counts_df = pd.read_excel(net_file, sheetname='Counts Output')\n",
    "\n",
    "    counts_df.drop([u'OBJECTID_1', u'Join_Count', u'TARGET_FID', u'OBJECTID', u'SR', u'RID', \n",
    "             u'MP', u'ARM', u'Type_', u'Lanes', u'Oneway', u'Dir', u'ID',\n",
    "             u'HOV_I', u'HOV_J'],\n",
    "           axis=1, inplace=True)\n",
    "    \n",
    "    # Model results\n",
    "    df = counts_df\n",
    "    df = df[['vol'+str(i) for i in tod_list]+['NewINode']]\n",
    "    df = df.set_index(keys='NewINode',drop=True)\n",
    "\n",
    "    # realign\n",
    "    df = pd.DataFrame(df.stack())\n",
    "\n",
    "    df['tod'] = df.index.get_level_values(1)\n",
    "    df['tod'] = df['tod'].apply(lambda row: row.split('vol')[-1])\n",
    "    df['NewINode'] = df.index.get_level_values(0)\n",
    "    df.rename(columns={0:'volume'}, inplace=True)\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    df['source'] = fname.split('.xlsx')[0]\n",
    "    \n",
    "    fname_out = 'traffic_counts.csv'\n",
    "    write_csv(df=df, fname=fname_out)\n",
    "\n",
    "    # Observed results\n",
    "    if 'observed' not in pd.read_csv(os.path.join(output_dir,fname_out))['source'].values:\n",
    "    \n",
    "        df = counts_df\n",
    "        for i in xrange(24):\n",
    "            if i < 10:\n",
    "                df = df.rename(columns={'Vol_0'+str(i): str(i)})\n",
    "            df = df.rename(columns={'Vol_'+str(i): str(i)})\n",
    "\n",
    "        df = df[[str(i) for i in xrange(24)]+['NewINode']]\n",
    "        df = df.set_index(keys='NewINode',drop=True)\n",
    "\n",
    "        # realign\n",
    "        df = pd.DataFrame(df.stack())\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'level_1':'hour',0:'volume'}, inplace=True)\n",
    "\n",
    "        # Load tod lookup\n",
    "        tod_df = pd.DataFrame(data=tod_lookup.values(),index=tod_lookup.keys(), columns=['tod'])\n",
    "        tod_df['hour'] = tod_df.index.astype('str')\n",
    "\n",
    "        df = pd.merge(df,tod_df,on='hour')\n",
    "        df.drop('hour', axis=1,inplace=True)\n",
    "\n",
    "        # Group by tod\n",
    "        df = df.groupby(['tod','NewINode']).sum()\n",
    "        df['tod'] = df.index.get_level_values(0)\n",
    "        df['NewINode'] = df.index.get_level_values(1)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        df['source'] = 'observed'\n",
    "\n",
    "        write_csv(df=df, fname=fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net_summary(net_file, fname):\n",
    "    \n",
    "    net_summary_df = pd.read_excel(net_file, sheetname='Network Summary')\n",
    "    net_summary_df.index = net_summary_df['tod']\n",
    "    df = pd.DataFrame(net_summary_df.stack())\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={0:'value','level_1':'fieldname'}, inplace=True)\n",
    "\n",
    "    # Drop the rows with TP_4k column headers\n",
    "    df.drop(df[df['fieldname'] == 'TP_4k'].index, inplace=True)\n",
    "    df.drop(df[df['fieldname'] == 'tod'].index, inplace=True)\n",
    "\n",
    "    # Split the fields by vmt, vht, delay\n",
    "    df['facility_type'] = df.fieldname.apply(lambda row: row.split('_')[0])\n",
    "    df['metric'] = df.fieldname.apply(lambda row: row.split('_')[-1])\n",
    "\n",
    "    df['source'] = fname.split('.xlsx')[0]\n",
    "    \n",
    "    write_csv(df=df, fname='net_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing latest_run_2014.xlsx\n",
      "processing test_branch_14.xlsx\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
